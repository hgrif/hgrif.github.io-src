[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Henk Griffioen",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "posts/start-data-science-project-python/index.html",
    "href": "posts/start-data-science-project-python/index.html",
    "title": "How to start a Data Science project in Python",
    "section": "",
    "text": "A lot of blog posts are written on the complicated Data Science-y stuff but not so many posts talk about the simple stuff. A simple but very important topic is how to start and structure your projects. This post gives a few pointers for setting up your projects."
  },
  {
    "objectID": "posts/start-data-science-project-python/index.html#project-structure",
    "href": "posts/start-data-science-project-python/index.html#project-structure",
    "title": "How to start a Data Science project in Python",
    "section": "Project structure",
    "text": "Project structure\nProject structures often organically grow to suit people’s needs, leading to different project structures within a team. You can consider yourself lucky if at some point in time you, or someone in your team, finds an obscure blog post with a somewhat sane structure and enforces it in your team.\nMany years ago I stumbled upon ProjectTemplate for R. Since then I’ve tried to get people to use a good project structure. More recently DrivenData released their more generic Cookiecutter Data Science.\nThe main philosophies of those projects are:\n\nA consistent and well organized structure allows people to collaborate more easily.\nYour analyses should be reproducible and your structure should enable that.\nA projects starts from raw data that should never be edited; consider raw data immutable and only edit derived sources.\n\nI couldn’t help to invent my own project structure and my minimal structure looks something like this (example here):\nexample_project/\n├── data/               &lt;- The original, immutable data dump.\n├── figures/            &lt;- Figures saved by notebooks and scripts.\n├── notebooks/          &lt;- Jupyter notebooks.\n├── output/             &lt;- Processed data, models, logs, etc.\n├── exampleproject/     &lt;- Python package with source code.\n│   └── __init__.py     &lt;-- Make the folder a package.\n    └── process.py      &lt;-- Example module.\n├── tests/              &lt;- Tests for your Python package.\n    └── test_process.py &lt;-- Tests for process.py.\n├── environment.yml     &lt;- Virtual environment definition.\n├── README.md           &lt;- README with info of the project.\n└── setup.py            &lt;- Install and distribute your module.\nIt mostly follows the other structures:\n\nraw data is immutable and goes to data/;\nprocessed data and derived output goes to different folders like figures/ and output/;\nnotebooks go to notebooks/;\nproject info goes in the README.md;\nand the project code goes to a separate folder.\n\nI try to make a full-fledged Python package (plus tests) out of my project structure so that the step between prototyping and production is as small as possible. The setup.py allows me to install the package in a virtual environment and use it in my Notebooks (more on this in a later blog post).\nIt doesn’t really matter which structure you pick, as long as it fits your workflow and you stick with it for a while. Try to understand the philosophies of the projects and pick the structure that suits your needs."
  },
  {
    "objectID": "posts/start-data-science-project-python/index.html#virtual-environment",
    "href": "posts/start-data-science-project-python/index.html#virtual-environment",
    "title": "How to start a Data Science project in Python",
    "section": "Virtual environment",
    "text": "Virtual environment\nProjects should be independent of each other: you don’t want your new experiments to mess up your older work. We do this partly by putting the files of different projects in different folders but you should also use separate Python environments.\nVirtual environments are isolated environments that separate dependencies of different projects and avoid package conflicts. Each virtual environment has its own packages and its own package versions. Environment A can have numpy version 1.11 and pandas version 0.18 while environment B only has pandas version 0.17. I like conda virtual environments because they’re well suited for Data Science (read here why).\nCreate a new conda virtual environment called example-project with Python 3.5:\n$ conda install --name example-project python=3.5\nMake sure your virtual environment is activated (leave out the source if you’re on Window):\n$ source activate example-project\n… and you’re now ready to install your favourite packages!\n$ conda install pandas numpy jupyter scikit-learn\nWhen you’re switching to a different project, run a source deactivate and activate the project’s virtual environment.\nOnce you get the hang of the activate-deactivate-flow, you’ll find that a virtual environments is a lightweight tool to keep your Python environments separated. By exporting your environment definition file (i.e. all installed packages and their versions) your projects will also be easily reproducible. If you want a more detailed discussion, check Tim Hopper’s post."
  },
  {
    "objectID": "posts/start-data-science-project-python/index.html#git",
    "href": "posts/start-data-science-project-python/index.html#git",
    "title": "How to start a Data Science project in Python",
    "section": "Git",
    "text": "Git\nEvery project should have its own Git repository. Having a repo per project allows you to track the history of a project and maintain complex version dependencies between projects.\nAlternatively, you can choose to have one repository with multiple projects, putting all the knowledge in a single place. The downside is, however, that it often ends up with ugly merge conflicts: Data Scientists are generally not that fluent with Git. In addition to a lot of Git frustrations, it makes your projects less independent of eachother.\nThe easiest way to set up Git is by creating a new git repository on your Git host (e.g. GitHub or GitLab) and cloning that:\n$ git clone https://github.com/hgrif/\nYou can then setup your project structure in this empty folder.\nIf you followed this guide and already created a folder with some files, first initialize a git repository on your machine:\n$ git init\nThen create a new git repository on your host, get its link and run:\n$ git remote add origin https://github.com/hgrif/\nThis adds the remote repository with the link https://github.com/hgrif/ and names it origin. You probably have to push your current master branch to origin:\n$ git push --set-upstream origin master\nNow that Git is set up, you can git add and git commit to your heart’s content!"
  },
  {
    "objectID": "posts/start-data-science-project-python/index.html#tooling",
    "href": "posts/start-data-science-project-python/index.html#tooling",
    "title": "How to start a Data Science project in Python",
    "section": "Tooling",
    "text": "Tooling\nYou can get away of some of the repetitive tasks by using some tooling!\nThe Python package cookiecutter automatically creates project folders based on a template. You can use existing template like the Cookiecutter Data Science or mine, or invent your own.\nThe easiest way to use virtual environments is to use an editor like PyCharm that supports them. You can also use autoenv or direnv to activate a virtual environment and set environment variables if you cd into a directory."
  },
  {
    "objectID": "posts/start-data-science-project-python/index.html#conclusion",
    "href": "posts/start-data-science-project-python/index.html#conclusion",
    "title": "How to start a Data Science project in Python",
    "section": "Conclusion",
    "text": "Conclusion\nHaving a good setup for your Data Science projects makes it easier for other people to work on your projects and makes them more reproducible. A good structure, a virtual environment and a git repository are the building blocks of any project."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Writing",
    "section": "",
    "text": "Whitepaper: MLOps Beyond the Hype [xebia.com]\n\n\n\n\n\n\n\n\nApr 1, 2024\n\n\n\n\n\n\n\nHow to Measure Your MLOps Performance [xebia.com]\n\n\n\n\n\n\n\n\nNov 20, 2023\n\n\n\n\n\n\n\ndbt tutorial: analytics engineering made easy [xebia.com]\n\n\n\n\n\n\n\n\nFeb 12, 2020\n\n\n\n\n\n\n\nRhyme with AI [xebia.com]\n\n\n\n\n\n\n\n\nFeb 8, 2020\n\n\n\n\n\n\n\nFairness in AI [xebia.com]\n\n\n\n\n\n\n\n\nJul 23, 2019\n\n\n\n\n\n\n\nWhitepaper: Analytics Translator [xebia.com]\n\n\n\n\n\n\n\n\nApr 1, 2019\n\n\n\n\n\n\n\nWrite less terrible code with Jupyter Notebook [xebia.com]\n\n\n\n\n\n\n\n\nAug 5, 2018\n\n\n\n\n\n\n\nFairness in Machine Learning with PyTorch [xebia.com]\n\n\n\n\n\n\n\n\nMay 22, 2018\n\n\n\n\n\n\n\nHandcrafting Recurrent Neural Networks to count\n\n\n\n\n\n\n\n\nNov 27, 2017\n\n\n\n\n\n\n\nAirflow Tutorial\n\n\n\n\n\n\n\n\nAug 11, 2017\n\n\n\n\n\n\n\nApache Airflow Tutorial for Data Pipelines [xebia.com]\n\n\n\n\n\n\n\n\nAug 11, 2017\n\n\n\n\n\n\n\nHow to Start a Data Science Project in Python [xebia.com]\n\n\n\n\n\n\n\n\nMar 1, 2017\n\n\n\n\n\n\n\nHow to start a Data Science project in Python\n\n\n\n\n\n\n\n\nFeb 26, 2017\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "About me"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "About me"
  },
  {
    "objectID": "posts/handcrafting-rnns-to-count/index.html",
    "href": "posts/handcrafting-rnns-to-count/index.html",
    "title": "Handcrafting Recurrent Neural Networks to count",
    "section": "",
    "text": "Recurrent neural networks (RNNs) handle complex problems like convert speech to text like a pro. This blog shows the basics of RNNs by teaching a RNN to count.\nRNNs are well suited for problems with sequences like captioning images and converting speech to text. The former generates an sequence of words from an image, the latter translate a stream of audio into words.\nState of the art RNNs can choose to remember or forget a part of a sequence, and can learn to which part to attend to. Some fantastic blogs on this topic are The Unreasonable Effectiveness of Recurrent Neural Networks, Understanding LSTM Networks and Attention and Augmented Recurrent Neural Networks.\nI wrote this blog while trying to reproduce some of the results of Exploring LSTMs. Let’s first go back to the basics!"
  },
  {
    "objectID": "posts/handcrafting-rnns-to-count/index.html#the-basics",
    "href": "posts/handcrafting-rnns-to-count/index.html#the-basics",
    "title": "Handcrafting Recurrent Neural Networks to count",
    "section": "1. The basics",
    "text": "1. The basics\nConsider the situation where we start of with \\(\\mathbf{x}\\), some image data, and \\(\\mathbf{y}\\), labels indicating if the image does or does not contain a dog. Our neural network should do some magical operations to find all the dogs in the images.\n\nFeedforward nets\nThe most basic neural network is a feedforward network, shown in the figure below. On the left, a network with one layer with two hidden units. On the right, a compact notation of the left image representing a network with an arbitrary number of hidden units.\n\n\n\nThis network adds constant values to the input \\(\\mathbf{x}\\), weights the result and passes this through some function \\(f\\). This gives use the activations \\(\\mathbf{h}\\) in the hidden layer:\n\\[\\mathbf{h}=f \\left(\\mathbf{W}^T \\mathbf{x}  + \\mathbf{b} \\right)\\]\nTo get our predictions \\(\\hat{\\mathbf{y}}\\) we’ll do the same with \\(\\mathbf{h}\\):\n\\[\\hat{\\mathbf{y}}=g \\left(\\mathbf{w}^T \\mathbf{h} + \\mathbf{c} \\right)\\]\nLearning is the process of optimizing the parameters \\(\\mathbf{W}\\), \\(\\mathbf{b}\\), \\(\\mathbf{w}\\) and \\(\\mathbf{c}\\) so that we get correct predictions.\nBefore learning, the network starts with some random values for its parameters. The data \\(\\mathbf{X}\\) is fed forward through our functions, resulting in predictions. The predictions are compared to the ground truth and the error is backpropagated through the network so we can find better values for our values. Doing this many times will (hopefully) teach the network to recognize dogs.\nDeep Learning is the process of finding the architecture of the network (e.g. how many hidden layers & units to use) and teaching it to learn. This process often involves waiting, throwing a lot of money on GPU’s and burning out PhD students. Deep Learning is not for the faint of heart.\nThe network in the figure above does not really deal well with sequences. Let’s say you get one of these ambiguous images:\n\n\n\nIf you just saw a picture of a tiny dog house, you’re probably more likely to think that the weird object in the picture is a chihuahua and not a muffin. This makes sense: context matters.\nOur network learns its parameters once and has a fixed state, so it cannot take context into account. It’s opinion doesn’t change depending on what it just saw. We’ll have to find a network architecture that can remember.\n\n\nRecurrent neural nets\nA recurrent neural network updates an internal state based on what it has seen so far. A diagram is shown below.\n\n\n\n\\(\\mathbf{x}\\) is now a sequence of data for multiple time steps \\(t\\). A sequence can consist, for exampe, of images, words or phrases uttered. At any given time \\(t\\), we construct an idea \\(\\mathbf{h}^{(t)}\\) from \\(\\mathbf{x}^{(t)}\\), our new input, and \\(\\mathbf{h}^{(t-1)}\\), our ideas so far. For the formula-minded audience:\n\\[ \\mathbf{h}^{(t)} =f \\left(\\mathbf{h}^{(t-1)}, \\mathbf{x}^{(t)}; \\boldsymbol{\\theta} \\right) \\]\n(All the parameters for this layer are put in \\(\\boldsymbol{\\theta}\\).)\nThis specific architecture waits for the whole sequence to end, but there are also forms that generate output for each time step. Similarly in real life, we can wait for someone to finish her sentence before translating it or try to translate someone on the fly. We’ll learn a neural network to count with the former approach."
  },
  {
    "objectID": "posts/handcrafting-rnns-to-count/index.html#the-counting-problem",
    "href": "posts/handcrafting-rnns-to-count/index.html#the-counting-problem",
    "title": "Handcrafting Recurrent Neural Networks to count",
    "section": "2. The counting problem",
    "text": "2. The counting problem\nThis counting problem will show that a RNN is able to keep a state. Our RNN will see a sequence of a’s and has to output the same number of b’s. Counting for this tasks means keeping track of how many a’s it has seen and the b’s outputted so far.\nThe data looks like this:\n['axb',\n 'aaxbb',\n 'aaaxbbb',\n 'aaaaxbbbb',\n 'aaaaaxbbbbb',\n 'aaaaaaxbbbbbb',\n 'aaaaaaaxbbbbbbb',\n 'aaaaaaaaxbbbbbbbb',\n 'aaaaaaaaaxbbbbbbbbb',\n 'aaaaaaaaaaxbbbbbbbbbb',\n 'aaaaaaaaaaaxbbbbbbbbbbb',\n 'aaaaaaaaaaaaxbbbbbbbbbbbb',\n 'aaaaaaaaaaaaaxbbbbbbbbbbbbb',\n 'aaaaaaaaaaaaaaxbbbbbbbbbbbbbb']\nHere we have:\n\naaa: the sequence to count;\nx: a switch character telling that the sequence has ended and prediction should start;\nbbb: the sequence to output.\n\nWe’ll add another special character: s. s signals that the sequence has ended and is also used for padding so that all our sequences are of the same length."
  },
  {
    "objectID": "posts/handcrafting-rnns-to-count/index.html#preprocessing",
    "href": "posts/handcrafting-rnns-to-count/index.html#preprocessing",
    "title": "Handcrafting Recurrent Neural Networks to count",
    "section": "3. Preprocessing",
    "text": "3. Preprocessing\nWe’ll generate all possible combinations of sentences and the next character to be predicted by moving through the text. The network is asked to predict b’s after x, for example:\nText: aaxbb\nSentences: ['aax', 'aaxb', 'aaxbb']\nNext char: ['b', 'b', 's']\nWe’ll have to vectorize the text into numerical features so that keras can use it.\nElement of X:\n        a      b      x      s\n0    True  False  False  False\n1    True  False  False  False\n2   False  False   True  False\n3   False   True  False  False\n4   False  False  False  False\n...\n26  False  False  False  False\n27  False  False  False  False\n28  False  False  False  False\n29  False  False  False  False\n30  False  False  False  False\nElement of y:\nTrue\nIn this example, the columns correspond with a, b, x or s and the rows are the characters of the sequence (note that we’re missing padding in this example). Our X consists of matrices stacked like this. An element of y is a single boolean value indicating if a b should be predicted."
  },
  {
    "objectID": "posts/handcrafting-rnns-to-count/index.html#the-network",
    "href": "posts/handcrafting-rnns-to-count/index.html#the-network",
    "title": "Handcrafting Recurrent Neural Networks to count",
    "section": "4. The network",
    "text": "4. The network\nWhat kind of architecture should our model have?\nWe’ll feed in X and want to predict for a single class (b or not b). The last layer should thus be a single node dense layer with sigmoid activation, but how is the network going to count?\nAs this is a fairly simple task, a plain old RNN should be good enough:\n\n\n\nOur RNN should keep track of the number a’s seen and b’s outputted so far and decide whether to output another b.\n\nHandcrafting\nWe can let the model learn, but we can also handcraft the neural network ourself! Our RNN should increment a counter for every a it has seen and increase a different counter for every b it has predicted. A b should only be predicted if the count for observed a’s is higher than for predicted b’s.\nThe RNN should thus consist of two units with a linear activation function. The update equations for our neural network are:\n\\[\\mathbf{h}^{(t)}_{RNN}=\\mathbf{b}_{RNN} + \\mathbf{W}_{RNN} \\mathbf{h}^{(t-1)}_{RNN} + \\mathbf{U}_{RNN} \\mathbf{x}^{(t)}\\]\n\\[\\mathbf{y}=\\mathsf{sigmoid} \\left( \\mathbf{b}_{dense} + \\mathbf{W}_{dense} \\mathbf{h}^{(T)}_{RNN} \\right)\\]\nWe can create this network in keras with:\nn_remember_units = 2\n\nsequence_input = layers.Input(X.shape[1:])\nx = layers.SimpleRNN(n_remember_units, activation='linear',\n                     name='rnn')(sequence_input)\nx = layers.Dense(1, activation='sigmoid', name='dense')(x)\ncounting_model = models.Model(sequence_input, x)\nEach unit focusses on one character: \\(\\mathbf{U}_{RNN}\\) is only non-zero if an a or b is seen for its unit. The units should only look at its own previous states, so \\(\\mathbf{W}_{RNN}\\) is a diagonal matrix with ones. Choosing a value of one for these matrices will add the input (if it’s b or not) to the number of b’s seen.\nThe dense layer substracts the input of the b unit from the a unit, and converts it to a probability. We’ll weigh the counts coming from the RNNs with +0.5 for a and -0.5 for b.\nThe probability will be higher than 0.5 if n_a &gt; n_b. All biases are zero. (Question for the reader: why do we need \\(&gt; 0.5\\) and not \\(\\geq 0.5\\)?).\nweights = [\n    np.array([[1, 0],\n              [0, 1],\n              [0, 0],\n              [0, 0]\n             ]),  # U_RNN\n    np.array([[1, 0],\n              [0, 1]]),  # W_RNN\n    np.array([0, 0]),  # b_RNN\n    np.array([[0.5], [-0.5]]),  # W_dense\n    np.array([0])  # b_dense\n]\n\ncounting_model.set_weights(weights)  # No .compile() and .fit() needed!\nWill this really work?\nfrom sklearn.metrics import classification_report\n\n\ny_pred = counting_model.predict(X) &gt; 0.5\nprint(classification_report(y_pred, y))\n             precision    recall  f1-score   support\n\n      False       1.00      1.00      1.00       196\n       True       1.00      1.00      1.00       105\n\navg / total       1.00      1.00      1.00       301\nYes, it works!\nLet’s investigate what’s happening inside the RNN. To see what the the RNN is doing, we can discard the last classification layer and look into the RNN layer. Create a new model without the dense layer and tell the RNN layer to return the full sequences instead of only the last hidden state:\nsequence_input = layers.Input(X.shape[1:])\nx = layers.SimpleRNN(n_remember_units, return_sequences=True,\n                     weights=counting_model.layers[1].get_weights())(sequence_input)\nhidden_model = models.Model(sequence_input, x)\nCalling hidden_model.predict(X) will give use the full sequence. Let’s look at what the hidden states are for a full sequence:\n\n\n\nThe hidden state of cell 0 slowly increases as it sees more a’s. If no a’s are present, the weighted average of the hidden state and the input (0) slowly decreases its value. The same happens for cell 1 that’s looking at b’s. The output of cell 1 is substracted from the output of cell 0 and comparing it to 0.5 indicates if another b should be outputted. This allows our RNN to perfectly solve this problem!"
  },
  {
    "objectID": "posts/handcrafting-rnns-to-count/index.html#conclusion",
    "href": "posts/handcrafting-rnns-to-count/index.html#conclusion",
    "title": "Handcrafting Recurrent Neural Networks to count",
    "section": "4. Conclusion",
    "text": "4. Conclusion\nWe solved the problem, but did our RNN learn how to count? Not really: our RNN perfectly solves the problem but does not understand the concept of counting. The RNN can hold states and can compare them but it did not solve the underlying problem. Nevertheless, this blog (hopefully) demonstrated that RNNs can hold state and this can be used for far more complex problems!"
  },
  {
    "objectID": "posts/airflow-tutorial/index.html",
    "href": "posts/airflow-tutorial/index.html",
    "title": "Airflow Tutorial",
    "section": "",
    "text": "Airflow is a scheduler for workflows such as data pipelines, similar to Luigi and Oozie.\nThis tutorial is loosely based on the Airflow tutorial in the official documentation. It will walk you through the basics of setting up Airflow and creating an Airflow workflow, and it will give you some practical tips. A (possibly) more up-to-date version of this blog can be found in my git repo."
  },
  {
    "objectID": "posts/airflow-tutorial/index.html#setup",
    "href": "posts/airflow-tutorial/index.html#setup",
    "title": "Airflow Tutorial",
    "section": "1. Setup",
    "text": "1. Setup\nSetting up a basic configuration of Airflow is pretty straightforward. After installing the Python package, we’ll need a database to store some data and start the core Airflow services.\nYou can skip this section if Airflow is already set up. Make sure that you can run airflow commands, know where to put your DAGs and have access to the web UI.\n\nInstall Airflow\nAirflow is installable with pip via a simple pip install apache-airflow. Either use a separate Python virtual environment or install it in your default python environment.\nTo use the conda virtual environment as defined in environment.yml from my git repo:\n\nInstall miniconda.\nMake sure that conda is on your path:\n\n$ which conda\n~/miniconda3/bin/conda\n\nCreate the virtual environment from environment.yml:\n\n$ conda env create -f environment.yml\n\nActivate the virtual environment:\n\n$ source activate airflow-tutorial\nYou should now have an (almost) working Airflow installation.\nAlternatively, install Airflow yourself by running:\n$ pip install apache-airflow\nAirflow used to be packaged as airflow but is packaged as apache-airflow since version 1.8.1. Make sure that you install any extra packages with the right Python package: e.g. use pip install apache-airflow[dask] if you’ve installed apache-airflow and do not use pip install airflow[dask]. Leaving out the prefix apache- will install an old version of Airflow next to your current version, leading to a world of hurt.\nYou may run into problems if you don’t have the right binaries or Python packages installed for certain backends or operators. When specifying support for e.g. PostgreSQL when installing extra Airflow packages, make sure the database is installed; do a brew install postgresql or apt-get install postgresql before the pip install apache-airflow[postgres]. Similarly, when running into HiveOperator errors, do a pip install apache-airflow[hive] and make sure you can use Hive.\n\n\nRun Airflow\nBefore you can use Airflow you have to initialize its database. The database contains information about historical & running workflows, connections to external data sources, user management, etc. Once the database is set up, Airflow’s UI can be accessed by running a web server and workflows can be started.\nThe default database is a SQLite database, which is fine for this tutorial. In a production setting you’ll probably be using something like MySQL or PostgreSQL. You’ll probably want to back it up as this database stores the state of everything related to Airflow.\nAirflow will use the directory set in the environment variable AIRFLOW_HOME to store its configuration and our SQlite database. This directory will be used after your first Airflow command. If you don’t set the environment variable AIRFLOW_HOME, Airflow will create the directory ~/airflow/ to put its files in.\nSet environment variable AIRFLOW_HOME to e.g. your current directory $(pwd):\n# change the default location ~/airflow if you want:\n$ export AIRFLOW_HOME=\"$(pwd)\"\nor any other suitable directory.\nNext, initialize the database:\n$ airflow initdb\nNow start the web server and go to localhost:8080 to check out the UI:\n$ airflow webserver --port 8080\nIt should look something like this:\n\nWith the web server running workflows can be started from a new terminal window. Open a new terminal, activate the virtual environment and set the environment variable AIRFLOW_HOME for this terminal as well:\n$ source activate airflow-tutorial\n$ export AIRFLOW_HOME=\"$(pwd)\"\nMake sure that you’re an in the same directory as before when using $(pwd).\nRun a supplied example:\n$ airflow run example_bash_operator runme_0 2017-07-01\nAnd check in the web UI that it has run by going to Browse -&gt; Task Instances.\nThis concludes all the setting up that you need for this tutorial.\n\n\nTips\n\nBoth Python 2 and 3 are be supported by Airflow. However, some of the lesser used parts (e.g. operators in contrib) might not support Python 3.\nFor more information on configuration check the sections on Configuration and Security of the Airflow documentation.\nCheck the Airflow repository for upstart and systemd templates.\nAirflow logs extensively, so pick your log folder carefully.\nSet the timezone of your production machine to UTC: Airflow assumes it’s UTC."
  },
  {
    "objectID": "posts/airflow-tutorial/index.html#workflows",
    "href": "posts/airflow-tutorial/index.html#workflows",
    "title": "Airflow Tutorial",
    "section": "2. Workflows",
    "text": "2. Workflows\nWe’ll create a workflow by specifying actions as a Directed Acyclic Graph (DAG) in Python. The tasks of a workflow make up a Graph; the graph is Directed because the tasks are ordered; and we don’t want to get stuck in an eternal loop so the graph also has to be Acyclic.\nThe figure below shows an example of a DAG:\n\nThe DAG of this tutorial is a bit easier. It will consist of the following tasks:\n\nprint 'hello'\nwait 5 seconds\nprint 'world\n\nand we’ll plan daily execution of this workflow.\n\nCreate a DAG file\nGo to the folder that you’ve designated to be your AIRFLOW_HOME and find the DAGs folder located in subfolder dags/ (if you cannot find, check the setting dags_folder in $AIRFLOW_HOME/airflow.cfg). Create a Python file with the name airflow_tutorial.py that will contain your DAG. Your workflow will automatically be picked up and scheduled to run.\nFirst we’ll configure settings that are shared by all our tasks. Settings for tasks can be passed as arguments when creating them, but we can also pass a dictionary with default values to the DAG. This allows us to share default arguments for all the tasks in our DAG is the best place to set e.g. the owner and start date of our DAG.\nAdd the following import and dictionary to airflow_tutorial.py to specify the owner, start time, and retry settings that are shared by our tasks:\n\n\nConfigure common settings\n\nimport datetime as dt\n\ndefault_args = {\n    'owner': 'me',\n    'start_date': dt.datetime(2017, 6, 1),\n    'retries': 1,\n    'retry_delay': dt.timedelta(minutes=5),\n}\n\nThese settings tell Airflow that this workflow is owned by 'me', that the workflow is valid since June 1st of 2017, it should not send emails and it is allowed to retry the workflow once if it fails with a delay of 5 minutes. Other common default arguments are email settings on failure and the end time.\n\n\nCreate the DAG\nWe’ll now create a DAG object that will contain our tasks.\nName it airflow_tutorial_v01 and pass default_args:\n\nfrom airflow import DAG\n\nwith DAG('airflow_tutorial_v01',\n         default_args=default_args,\n         schedule_interval='0 * * * *',\n         ) as dag:\n\nWith schedule_interval='0 * * * *' we’ve specified a run at every hour 0; the DAG will run each day at 00:00. See crontab.guru for help deciphering cron schedule expressions. Alternatively, you can use strings like '@daily' and '@hourly'.\nWe’ve used a context manager to create a DAG (new since 1.8). All the tasks for the DAG should be indented to indicate that they are part of this DAG. Without this context manager you’d have to set the dag parameter for each of your tasks.\nAirflow will generate DAG runs from the start_date with the specified schedule_interval. Once a DAG is active, Airflow continuously checks in the database if all the DAG runs have successfully ran since the start_date. Any missing DAG runs are automatically scheduled. When you initialize on 2016-01-04 a DAG with a start_date at 2016-01-01 and a daily schedule_interval, Airflow will schedule DAG runs for all the days between 2016-01-01 and 2016-01-04.\nA run starts after the time for the run has passed. The time for which the workflow runs is called the execution_date. The daily workflow for 2016-06-02 runs after 2016-06-02 23:59 and the hourly workflow for 2016-07-03 01:00 starts after 2016-07-03 01:59.\nFrom the ETL viewpoint this makes sense: you can only process the daily data for a day after it has passed. This can, however, ask for some juggling with date for other workflows. For Machine Learning models you may want to use all the data up to a given date, you’ll have to add the schedule_interval to your execution_date somewhere in the workflow logic.\nBecause Airflow saves all the (scheduled) DAG runs in its database, you should not change the start_date and schedule_interval of a DAG. Instead, up the version number of the DAG (e.g. airflow_tutorial_v02) and avoid running unnecessary tasks by using the web interface or command line tools\nTimezones and especially daylight savings can mean trouble when scheduling things, so keep your Airflow machine in UTC. You don’t want to skip an hour because daylight savings kicks in (or out).\n\n\nCreate the tasks\nTasks are represented by operators that either perform an action, transfer data, or sense if something has been done. Examples of actions are running a bash script or calling a Python function; of transfers are copying tables between databases or uploading a file; and of sensors are checking if a file exists or data has been added to a database.\nWe’ll create a workflow consisting of three tasks: we’ll print ‘hello’, wait for 10 seconds and finally print ‘world’. The first two are done with the BashOperator and the latter with the PythonOperator. Give each operator an unique task ID and something to do:\n\n    from airflow.operators.bash_operator import BashOperator\n    from airflow.operators.python_operator import PythonOperator\n\n    def print_world():\n        print('world')\n\n    print_hello = BashOperator(task_id='print_hello',\n                               bash_command='echo \"hello\"')\n    sleep = BashOperator(task_id='sleep',\n                         bash_command='sleep 5')\n    print_world = PythonOperator(task_id='print_world',\n                                 python_callable=print_world)\n\nNote how we can pass bash commands in the BashOperator and that the PythonOperator asks for a Python function that can be called.\nDependencies in tasks are added by setting other actions as upstream (or downstream). Link the operations in a chain so that sleep will be run after print_hello and is followed by print_world; print_hello -&gt; sleep -&gt; print_world:\n\nprint_hello &gt;&gt; sleep &gt;&gt; print_world\n\nAfter rearranging the code your final DAG should look something like:\n\nimport datetime as dt\n\nfrom airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators.python_operator import PythonOperator\n\n\ndef print_world():\n    print('world')\n\n\ndefault_args = {\n    'owner': 'me',\n    'start_date': dt.datetime(2017, 6, 1),\n    'retries': 1,\n    'retry_delay': dt.timedelta(minutes=5),\n}\n\n\nwith DAG('airflow_tutorial_v01',\n         default_args=default_args,\n         schedule_interval='0 * * * *',\n         ) as dag:\n\n    print_hello = BashOperator(task_id='print_hello',\n                               bash_command='echo \"hello\"')\n    sleep = BashOperator(task_id='sleep',\n                         bash_command='sleep 5')\n    print_world = PythonOperator(task_id='print_world',\n                                 python_callable=print_world)\n\n\nprint_hello &gt;&gt; sleep &gt;&gt; print_world\n\n\n\nTest the DAG\nFirst check that DAG file contains valid Python code by executing the file with Python:\n$ python airflow_tutorial.py\nYou can manually test a single task for a given execution_date with airflow test:\n$ airflow test airflow_tutorial_v01 print_world 2016-07-01\nThis runs the task locally as if it was for 2017-07-01, ignoring other tasks and without communicating to the database.\n\n\nActivate the DAG\nNow that you’re confident that your dag works, turn on the DAG in the web UI and sit back while Airflow starts backfilling the dag runs!\n\n\nTips\n\nMake your DAGs idempotent: rerunning them should give the game results.\nUse the the cron notation for schedule_interval instead of @daily and @hourly. @daily and @hourly always run after respectively midnight and the full hour, regardless of the hour/minute specified.\nManage your connections and secrets with the Connections and/or Variables."
  },
  {
    "objectID": "posts/airflow-tutorial/index.html#exercises",
    "href": "posts/airflow-tutorial/index.html#exercises",
    "title": "Airflow Tutorial",
    "section": "3. Exercises",
    "text": "3. Exercises\nYou now know the basics of setting up Airflow, creating a DAG and turning it on; time to go deeper!\n\nChange the interval to every 30 minutes.\nUse a sensor to add a delay of 5 minutes before starting.\nImplement templating for the BashOperator: print the execution_date instead of 'hello' (check out the original tutorial and the example DAG).\nUse templating for the PythonOperator: print the execution_date with one hour added in the function print_world() (check out the documentation of the PythonOperator)."
  },
  {
    "objectID": "posts/airflow-tutorial/index.html#resources",
    "href": "posts/airflow-tutorial/index.html#resources",
    "title": "Airflow Tutorial",
    "section": "4. Resources",
    "text": "4. Resources\n\nThe official Airflow tutorial: showing a bit more in-depth templating magic.\nETL best practices with Airflow: good best practices to follow when using Airflow.\nAirflow: Tips, Tricks, and Pitfalls: more explanations to help you grok Airflow."
  }
]